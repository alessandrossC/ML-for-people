{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrossC/ML-for-people/blob/main/%22%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rrjVE7QUdOXH",
        "outputId": "3120299c-e06b-46b7-d3e0-e32a0adc2853"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення на тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "# Вивід результатів\n",
        "print(\"Inputs Tensor:\\n\", inputs_tensor)\n",
        "print(\"\\nTargets Tensor:\\n\", targets_tensor)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8b6537-539b-4f3c-b22c-031ce01aff10"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Tensor:\n",
            " tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "\n",
            "Targets Tensor:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3044a5aa-ba23-47b5-e448-12fa86861d0a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bcf30302b50>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Розміри\n",
        "input_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Ініціалізуємо ваги \"навпаки\": (1, 3)\n",
        "w = torch.randn(output_size, input_size, requires_grad=True)  # (1,3)\n",
        "b = torch.randn(output_size, requires_grad=True)              # (1,)\n",
        "\n",
        "# Вивід результатів\n",
        "print(\"w (output_size x input_size):\\n\", w)\n",
        "print(\"\\nb:\\n\", b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4a5d8e-ed84-4ce7-ec69-efebc1fdb17e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w (output_size x input_size):\n",
            " tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "\n",
            "b:\n",
            " tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Функція моделі без сигмоїди\n",
        "def model(x, w, b):\n",
        "    return x @ w.t() + b\n",
        "\n",
        "\n",
        "predictions = model(inputs, w, b)\n",
        "\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25a42c5-6754-4b9f-f4b9-8f8ae89faec8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[69.4361],\n",
              "        [88.2410],\n",
              "        [97.5041],\n",
              "        [81.8390],\n",
              "        [76.1967]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Всі значення дуже негативні.\n",
        "Випадкова ініціалізація, відсутність навчання і нормалізації"
      ],
      "metadata": {
        "id": "WAkWT_M8g92Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Застосуємо сигмоїду до логітів\n",
        "predicted_probs = torch.sigmoid(predictions)\n",
        "\n",
        "# Обчислимо втрати\n",
        "loss = binary_cross_entropy(predicted_probs, targets)\n",
        "\n",
        "# Вивід результату\n",
        "print(f\"Втрати (loss): {loss.item():.4f}\")\n",
        "\n",
        "# Функція втрат\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    # Захист від log(0) — дуже важливо для стабільності\n",
        "    eps = 1e-7\n",
        "    predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
        "\n",
        "    # Формула бінарної крос-ентропії\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "\n",
        "    # Повертаємо середнє значення втрат\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4eb14d3-714f-4957-804c-16d7311c347e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Втрати (loss): 6.3770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Значення втрат 6.3770 — це досить велика втрата.\n",
        "В ідеалі для бінарної класифікації втрати прагнуть бути ближчими до 0.\n",
        "Чим більші втрати — тим гірше модель наближає передбачення до правильних міток."
      ],
      "metadata": {
        "id": "YbUEsaUAm5jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення градієнтів (зворотнє поширення)\n",
        "loss.backward()\n",
        "\n",
        "# Вивід градієнтів\n",
        "print(\"Градієнт ваг w:\\n\", w.grad)\n",
        "print(\"\\nГрадієнт зсуву b:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1437384-fe97-4f84-ac8f-c92fe0e37047"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Градієнт ваг w:\n",
            " tensor([[0., 0., 0.]])\n",
            "\n",
            "Градієнт зсуву b:\n",
            " tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predictions дуже великі і негативні"
      ],
      "metadata": {
        "id": "i9svS-TLoeBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція моделі (без сигмоїди, бо потім застосуємо вручну)\n",
        "def model(x, w, b):\n",
        "    return x @ w.t() + b\n",
        "\n",
        "# Передбачення логітів\n",
        "logits = model(inputs, w, b)\n",
        "\n",
        "# Застосовуємо сигмоїду\n",
        "predicted_probs = torch.sigmoid(logits)\n",
        "\n",
        "# Функція втрат\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    eps = 1e-7\n",
        "    predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "    return torch.mean(loss)\n",
        "\n",
        "# Обчислюємо втрати\n",
        "loss = binary_cross_entropy(predicted_probs, targets)\n",
        "\n",
        "# Зворотне поширення помилки\n",
        "loss.backward()\n",
        "\n",
        "# 11. Виводимо результати\n",
        "print(\"w:\\n\", w)\n",
        "print(\"\\nb:\\n\", b)\n",
        "print(\"\\nlogits:\\n\", logits)\n",
        "print(\"\\npredicted_probs:\\n\", predicted_probs)\n",
        "print(\"\\nloss:\\n\", loss)\n",
        "print(\"\\nГрадієнт ваг w.grad:\\n\", w.grad)\n",
        "print(\"\\nГрадієнт зміщення b.grad:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93574f24-5110-4c24-8c42-d8d2911705be"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w:\n",
            " tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "\n",
            "b:\n",
            " tensor([0.0006], requires_grad=True)\n",
            "\n",
            "logits:\n",
            " tensor([[0.0694],\n",
            "        [0.0882],\n",
            "        [0.0975],\n",
            "        [0.0818],\n",
            "        [0.0762]], grad_fn=<AddBackward0>)\n",
            "\n",
            "predicted_probs:\n",
            " tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "loss:\n",
            " tensor(0.6829, grad_fn=<MeanBackward0>)\n",
            "\n",
            "Градієнт ваг w.grad:\n",
            " tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "\n",
            "Градієнт зміщення b.grad:\n",
            " tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut6OKzPZqYYT",
        "outputId": "7387ac86-923c-4348-ff80-f6517ffd3bfc"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0694],\n",
            "        [0.0882],\n",
            "        [0.0975],\n",
            "        [0.0818],\n",
            "        [0.0762]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(t1, t2):\n",
        "    diff = t2 - t1\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "metadata": {
        "id": "Q3W0fy8MrrfP"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yBiW770qlHb",
        "outputId": "663e4c75-c887-4030-dd8d-868b2f77be52"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5021, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення градієнтів\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "EW1GXClzrwdx"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Оновлення ваг та ресетимо градієнти\n",
        "with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "XwHNtgGCsmmP"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldKX1MOdvI0r",
        "outputId": "898c983a-c1a8-4e85-97a0-11d1ecd1e7da"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0016, 0.0016, 0.0008]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPdCcjdtvqvj",
        "outputId": "e177c748-294a-4ab7-bc51-feb91e73ab33"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2801, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (1000):\n",
        "    preds = model (inputs, w, b)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7LAk-KqyIC-",
        "outputId": "7b098f6e-a867-413b-e115-9f0a9268a2f8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0547, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX4O7sWkzcJx",
        "outputId": "9c037ee2-8cf1-49ca-f1a0-b83167eccb97"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4338],\n",
              "        [ 0.7164],\n",
              "        [ 0.9619],\n",
              "        [-0.0465],\n",
              "        [ 1.0352]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhoilOimzggq",
        "outputId": "4c82a59e-50d3-4505-e7bc-b8f38726bafc"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Там, де таргет = 1, модель дає досить високу ймовірність (~0.67–0.73).\n",
        "Це добре.\n",
        "Там, де таргет = 0, модель поки що дає ~0.48–0.60. Це близько до 0.5, тобто ще не ідеально відрізняє класи."
      ],
      "metadata": {
        "id": "VpRarA_h0Pq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_t = torch.from_numpy(inputs)\n",
        "targets_t = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "niK8wzZY_8R0"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLUtDnMFALk-",
        "outputId": "c24ea633-bbbe-4276-c051-b9ae83f96bd8"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.],\n",
              "        [ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.],\n",
              "        [ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(inputs_t, targets_t)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73vTKMpP0tkL",
        "outputId": "ebed1521-af1d-4a93-a4c5-1d30b91119f3"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))\n"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b303900-509e-441d-dc66-f54b6cfc0d0d"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[102.,  43.,  37.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 1. Створення класу моделі логістичної регресії\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# 2. Створення екземпляру моделі\n",
        "input_size = 3   # Кількість ознак: temp, rainfall, humidity\n",
        "output_size = 1  # Один вихід (ймовірність 0 або 1)\n",
        "\n",
        "model = LogReg(input_size, output_size)\n",
        "\n",
        "# 3. Виведення моделі\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd26f35-fa78-43ca-9a7e-4c4daf5cec20"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "kCpZ4NAHFOpK"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення оптимізатора (SGD)\n",
        "opt = optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "NyAE3jPsFRfx"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція втрат\n",
        "loss_fn = F.mse_loss"
      ],
      "metadata": {
        "id": "3Ws1XWUOFbkj"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення втрат\n",
        "loss = loss_fn(model(inputs_t), targets_t)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd3307d-f208-4bb2-c9d3-4f4891431204"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1997, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель має випадкові початкові ваги.\n",
        "Модель ще не навчалась.\n",
        "Тому передбачення моделі сильно відрізняються від справжніх міток (targets)."
      ],
      "metadata": {
        "id": "Fi4J3F543axJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb, yb in train_dl:\n",
        "            # Створення передбачень\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконання градієнтного спуску\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "B2paJ7rHCrov"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "xU_6HFIX32ME"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Навчання моделі на 1000 епохах\n",
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWDLwTIh33jm",
        "outputId": "9f40cd7c-1c35-4252-a0fd-7f460fa204dc"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 0.1997\n",
            "Epoch [20/1000], Loss: 0.1997\n",
            "Epoch [30/1000], Loss: 0.1997\n",
            "Epoch [40/1000], Loss: 0.1997\n",
            "Epoch [50/1000], Loss: 0.1997\n",
            "Epoch [60/1000], Loss: 0.1997\n",
            "Epoch [70/1000], Loss: 0.1997\n",
            "Epoch [80/1000], Loss: 0.1997\n",
            "Epoch [90/1000], Loss: 0.1997\n",
            "Epoch [100/1000], Loss: 0.1997\n",
            "Epoch [110/1000], Loss: 0.1997\n",
            "Epoch [120/1000], Loss: 0.1997\n",
            "Epoch [130/1000], Loss: 0.1997\n",
            "Epoch [140/1000], Loss: 0.1997\n",
            "Epoch [150/1000], Loss: 0.1997\n",
            "Epoch [160/1000], Loss: 0.1997\n",
            "Epoch [170/1000], Loss: 0.1997\n",
            "Epoch [180/1000], Loss: 0.1997\n",
            "Epoch [190/1000], Loss: 0.1997\n",
            "Epoch [200/1000], Loss: 0.1997\n",
            "Epoch [210/1000], Loss: 0.1997\n",
            "Epoch [220/1000], Loss: 0.1997\n",
            "Epoch [230/1000], Loss: 0.1997\n",
            "Epoch [240/1000], Loss: 0.1997\n",
            "Epoch [250/1000], Loss: 0.1997\n",
            "Epoch [260/1000], Loss: 0.1997\n",
            "Epoch [270/1000], Loss: 0.1997\n",
            "Epoch [280/1000], Loss: 0.1997\n",
            "Epoch [290/1000], Loss: 0.1997\n",
            "Epoch [300/1000], Loss: 0.1997\n",
            "Epoch [310/1000], Loss: 0.1997\n",
            "Epoch [320/1000], Loss: 0.1997\n",
            "Epoch [330/1000], Loss: 0.1997\n",
            "Epoch [340/1000], Loss: 0.1997\n",
            "Epoch [350/1000], Loss: 0.1997\n",
            "Epoch [360/1000], Loss: 0.1997\n",
            "Epoch [370/1000], Loss: 0.1997\n",
            "Epoch [380/1000], Loss: 0.1997\n",
            "Epoch [390/1000], Loss: 0.1996\n",
            "Epoch [400/1000], Loss: 0.1996\n",
            "Epoch [410/1000], Loss: 0.1996\n",
            "Epoch [420/1000], Loss: 0.1996\n",
            "Epoch [430/1000], Loss: 0.1996\n",
            "Epoch [440/1000], Loss: 0.1996\n",
            "Epoch [450/1000], Loss: 0.1996\n",
            "Epoch [460/1000], Loss: 0.1996\n",
            "Epoch [470/1000], Loss: 0.1996\n",
            "Epoch [480/1000], Loss: 0.1996\n",
            "Epoch [490/1000], Loss: 0.1996\n",
            "Epoch [500/1000], Loss: 0.1996\n",
            "Epoch [510/1000], Loss: 0.1996\n",
            "Epoch [520/1000], Loss: 0.1996\n",
            "Epoch [530/1000], Loss: 0.1996\n",
            "Epoch [540/1000], Loss: 0.1996\n",
            "Epoch [550/1000], Loss: 0.1996\n",
            "Epoch [560/1000], Loss: 0.1996\n",
            "Epoch [570/1000], Loss: 0.1996\n",
            "Epoch [580/1000], Loss: 0.1996\n",
            "Epoch [590/1000], Loss: 0.1996\n",
            "Epoch [600/1000], Loss: 0.1996\n",
            "Epoch [610/1000], Loss: 0.1996\n",
            "Epoch [620/1000], Loss: 0.1996\n",
            "Epoch [630/1000], Loss: 0.1996\n",
            "Epoch [640/1000], Loss: 0.1996\n",
            "Epoch [650/1000], Loss: 0.1996\n",
            "Epoch [660/1000], Loss: 0.1996\n",
            "Epoch [670/1000], Loss: 0.1996\n",
            "Epoch [680/1000], Loss: 0.1996\n",
            "Epoch [690/1000], Loss: 0.1996\n",
            "Epoch [700/1000], Loss: 0.1996\n",
            "Epoch [710/1000], Loss: 0.1996\n",
            "Epoch [720/1000], Loss: 0.1996\n",
            "Epoch [730/1000], Loss: 0.1996\n",
            "Epoch [740/1000], Loss: 0.1996\n",
            "Epoch [750/1000], Loss: 0.1996\n",
            "Epoch [760/1000], Loss: 0.1996\n",
            "Epoch [770/1000], Loss: 0.1996\n",
            "Epoch [780/1000], Loss: 0.1996\n",
            "Epoch [790/1000], Loss: 0.1996\n",
            "Epoch [800/1000], Loss: 0.1996\n",
            "Epoch [810/1000], Loss: 0.1996\n",
            "Epoch [820/1000], Loss: 0.1996\n",
            "Epoch [830/1000], Loss: 0.1996\n",
            "Epoch [840/1000], Loss: 0.1996\n",
            "Epoch [850/1000], Loss: 0.1996\n",
            "Epoch [860/1000], Loss: 0.1996\n",
            "Epoch [870/1000], Loss: 0.1996\n",
            "Epoch [880/1000], Loss: 0.1996\n",
            "Epoch [890/1000], Loss: 0.1996\n",
            "Epoch [900/1000], Loss: 0.1996\n",
            "Epoch [910/1000], Loss: 0.1996\n",
            "Epoch [920/1000], Loss: 0.1996\n",
            "Epoch [930/1000], Loss: 0.1996\n",
            "Epoch [940/1000], Loss: 0.1996\n",
            "Epoch [950/1000], Loss: 0.1996\n",
            "Epoch [960/1000], Loss: 0.1996\n",
            "Epoch [970/1000], Loss: 0.1996\n",
            "Epoch [980/1000], Loss: 0.1996\n",
            "Epoch [990/1000], Loss: 0.1996\n",
            "Epoch [1000/1000], Loss: 0.1996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Побудова графіка Loss vs Epochs\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "PIUktxdB55Kx",
        "outputId": "f9f3e427-f82c-4db0-ae6b-5ce817cb1721"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs_t)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAbzs0RK6uP4",
        "outputId": "3b1b870b-a3b4-4643-ed61-a24627a65d16"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.9905e-01],\n",
              "        [9.9996e-01],\n",
              "        [1.0000e+00],\n",
              "        [7.2471e-06],\n",
              "        [1.0000e+00],\n",
              "        [9.9905e-01],\n",
              "        [9.9996e-01],\n",
              "        [1.0000e+00],\n",
              "        [7.2471e-06],\n",
              "        [1.0000e+00],\n",
              "        [9.9905e-01],\n",
              "        [9.9996e-01],\n",
              "        [1.0000e+00],\n",
              "        [7.2471e-06],\n",
              "        [1.0000e+00]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMP3Flmx65z8",
        "outputId": "7bcb50c5-a924-4e85-b5ca-46ccb4a66bd5"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ці передбачення точно відповідають цільовим міткам (targets), що свідчить про те, що модель навчилася правильно класифікувати всі зразки у наборі даних.​"
      ],
      "metadata": {
        "id": "avocPxDhM_M3"
      }
    }
  ]
}